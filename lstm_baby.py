#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LSTM è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ - è¯­è¨€å»ºæ¨¡è®­ç»ƒ
å‚è€ƒ colab_train_babydataset.py çš„ç»“æ„
åœ¨ baby_data/ ç›®å½•çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè®°å½•losså’Œperplexity
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import math
import os
import json
import time
from datetime import datetime
from tqdm import tqdm
import glob


# ========== é…ç½®éƒ¨åˆ† ==========


class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""

    # åŸºç¡€è·¯å¾„é…ç½®
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    DATA_DIR = os.path.join(BASE_DIR, "baby_data")
    RESULTS_DIR = os.path.join(BASE_DIR, "results_baby")
    LOG_FILE = os.path.join(BASE_DIR, "training_baby.log")

    # è®­ç»ƒé…ç½®
    BATCH_SIZE = 32
    NUM_EPOCHS = 10
    LEARNING_RATE = 0.001
    MAX_STEPS = 2000
    LOGGING_STEPS = 5
    SAVE_STEPS = 100

    # æ¨¡å‹é…ç½®
    EMBEDDING_DIM = 256
    HIDDEN_DIM = 512
    NUM_LAYERS = 2
    DROPOUT = 0.3
    MAX_LENGTH = 128

    # æ•°æ®é›†é…ç½®
    DATASETS = [
        {
            "name": "Natural Language",
            "data_dir": os.path.join(DATA_DIR, "natural"),
            "model_dir": os.path.join(RESULTS_DIR, "model_natural"),
            "file_pattern": "*.train"
        },
        {
            "name": "Impossible Language (Parity Negation)",
            "data_dir": os.path.join(DATA_DIR, "parity_negation"),
            "model_dir": os.path.join(RESULTS_DIR, "model_parity_negation"),
            "file_pattern": "*.train"
        },
        {
            "name": "Impossible Language (Reversed)",
            "data_dir": os.path.join(DATA_DIR, "reversed"),
            "model_dir": os.path.join(RESULTS_DIR, "model_reversed"),
            "file_pattern": "*.train"
        }
    ]

    @classmethod
    def create_directories(cls):
        """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
        directories = [cls.BASE_DIR, cls.DATA_DIR, cls.RESULTS_DIR]
        print("åˆ›å»ºç›®å½•ç»“æ„:")
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            print(f"  âœ“ {directory}")

        for dataset in cls.DATASETS:
            os.makedirs(dataset["model_dir"], exist_ok=True)
            print(f"  âœ“ {dataset['name']}: {dataset['model_dir']}")


# ========== æ—¥å¿—é…ç½® ==========


def setup_logging(log_file):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    log_dir = os.path.dirname(log_file)
    if log_dir and not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    if logger.handlers:
        logger.handlers.clear()

    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger


import logging


# ========== æ•°æ®é›†ç±» ==========


class LanguageModelingDataset(Dataset):
    """è¯­è¨€å»ºæ¨¡æ•°æ®é›†ç±» - æƒ°æ€§åŠ è½½ç‰ˆæœ¬"""

    def __init__(self, data_dir, file_pattern, vocab, max_length=128, max_samples=None):
        self.vocab = vocab
        self.max_length = max_length
        self.max_samples = max_samples

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        pattern = os.path.join(data_dir, file_pattern)
        self.data_files = glob.glob(pattern)

        if not self.data_files:
            raise FileNotFoundError(f"åœ¨ç›®å½• {data_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é… {file_pattern} çš„æ–‡ä»¶")

        # ç»Ÿè®¡æ€»è¡Œæ•°ï¼ˆä¸åŠ è½½å†…å®¹ï¼‰
        self.file_line_counts = []
        self.total_lines = 0
        for file_path in self.data_files:
            with open(file_path, 'r', encoding='utf-8') as f:
                line_count = sum(1 for _ in f)
            self.file_line_counts.append(line_count)
            self.total_lines += line_count

        # ä½¿ç”¨æ€»è¡Œæ•°ä½œä¸ºæ ·æœ¬æ•°ä¸Šé™
        self.num_samples = min(self.total_lines, max_samples) if max_samples else self.total_lines

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ–‡ä»¶å’Œè¡Œ
        file_idx = np.random.randint(0, len(self.data_files))
        file_path = self.data_files[file_idx]
        line_count = self.file_line_counts[file_idx]

        # éšæœºé€‰æ‹©ä¸€è¡Œ
        line_idx = np.random.randint(0, line_count)

        # è¯»å–æŒ‡å®šè¡Œ
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i == line_idx:
                    break

        line = line.strip()
        if not line:
            # å¦‚æœè¡Œä¸ºç©ºï¼Œè¿”å›ä¸€ä¸ªå¡«å……çš„æ ·æœ¬
            input_seq = [self.vocab['<PAD>']] * self.max_length
            target = self.vocab['<PAD>']
            return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)

        # åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰
        tokens = list(line)

        # è½¬æ¢ä¸ºç´¢å¼•
        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]

        # éšæœºé€‰æ‹©ä¸€ä¸ªä½ç½®ç”Ÿæˆè¾“å…¥-ç›®æ ‡å¯¹
        if len(indices) > 1:
            pos = np.random.randint(0, len(indices) - 1)
            input_seq = indices[:pos+1]
            target = indices[pos+1]
        else:
            input_seq = indices
            target = self.vocab['<UNK>']

        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        if len(input_seq) < self.max_length:
            input_seq = input_seq + [self.vocab['<PAD>']] * (self.max_length - len(input_seq))
        else:
            input_seq = input_seq[-self.max_length:]

        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)


# ========== LSTM è¯­è¨€æ¨¡å‹ ==========


class LSTMLanguageModel(nn.Module):
    """LSTM è¯­è¨€æ¨¡å‹"""

    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):
        super(LSTMLanguageModel, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        # x: [batch_size, seq_length]
        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]

        # LSTM å‰å‘ä¼ æ’­
        lstm_output, (hidden, cell) = self.lstm(embedded)

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_output[:, -1, :]  # [batch_size, hidden_dim]

        last_output = self.dropout(last_output)
        logits = self.fc(last_output)  # [batch_size, vocab_size]

        return logits


# ========== è®­ç»ƒç±» ==========


class LSTMLanguageModelTrainer:
    """LSTM è¯­è¨€æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, config, logger=None):
        self.config = config
        self.logger = logger
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.losses = []
        self.perplexities = []
        self.start_time = time.time()

    def build_vocab(self, data_dir, file_pattern, min_freq=2, max_lines=100000):
        """æ„å»ºè¯æ±‡è¡¨"""
        from collections import Counter

        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶
        pattern = os.path.join(data_dir, file_pattern)
        data_files = glob.glob(pattern)

        # ç»Ÿè®¡è¯é¢‘ï¼ˆé™åˆ¶æœ€å¤§è¡Œæ•°ä»¥èŠ‚çœå†…å­˜ï¼‰
        word_freq = Counter()
        lines_processed = 0

        for file_path in data_files:
            if lines_processed >= max_lines:
                break
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if lines_processed >= max_lines:
                        break
                    line = line.strip()
                    if line:
                        tokens = list(line)
                        word_freq.update(tokens)
                    lines_processed += 1

        if self.logger:
            self.logger.info(f"æ„å»ºè¯æ±‡è¡¨ï¼šå¤„ç†äº† {lines_processed} è¡Œï¼Œå‘ç° {len(word_freq)} ä¸ªå”¯ä¸€å­—ç¬¦")

        # æ„å»ºè¯æ±‡è¡¨
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, freq in word_freq.items():
            if freq >= min_freq:
                vocab[word] = len(vocab)

        vocab_size = len(vocab)
        if self.logger:
            self.logger.info(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

        return vocab

    def train(self, dataset_config):
        """è®­ç»ƒæ¨¡å‹"""

        dataset_name = dataset_config['name']
        data_dir = dataset_config['data_dir']
        model_dir = dataset_config['model_dir']
        file_pattern = dataset_config.get('file_pattern', '*.train')

        if self.logger:
            self.logger.info(f"å¼€å§‹è®­ç»ƒ: {dataset_name}")
            self.logger.info(f"æ•°æ®ç›®å½•: {data_dir}")

        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_dir):
            if self.logger:
                self.logger.error(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return None, None

        # æ„å»ºè¯æ±‡è¡¨
        vocab = self.build_vocab(data_dir, file_pattern)
        vocab_size = len(vocab)

        # åˆ›å»ºæ•°æ®é›†ï¼ˆé™åˆ¶æœ€å¤§æ ·æœ¬æ•°ä»¥èŠ‚çœå†…å­˜ï¼‰
        max_samples = min(50000, self.config.MAX_STEPS * self.config.BATCH_SIZE)
        dataset = LanguageModelingDataset(
            data_dir,
            file_pattern,
            vocab,
            max_length=self.config.MAX_LENGTH,
            max_samples=max_samples
        )

        if self.logger:
            self.logger.info(f"æ•°æ®é›†å¤§å°: {len(dataset)} æ ·æœ¬")

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_size = int(0.9 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.BATCH_SIZE,
            shuffle=True,
            num_workers=0
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.BATCH_SIZE,
            shuffle=False,
            num_workers=0
        )

        # åˆå§‹åŒ–æ¨¡å‹
        model = LSTMLanguageModel(
            vocab_size,
            self.config.EMBEDDING_DIM,
            self.config.HIDDEN_DIM,
            self.config.NUM_LAYERS,
            self.config.DROPOUT
        ).to(self.device)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
        optimizer = optim.Adam(model.parameters(), lr=self.config.LEARNING_RATE)

        # è®­ç»ƒå¾ªç¯
        global_step = 0
        best_val_loss = float('inf')

        for epoch in range(self.config.NUM_EPOCHS):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_loss = 0.0
            epoch_steps = 0

            for batch_inputs, batch_targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.config.NUM_EPOCHS}'):
                batch_inputs = batch_inputs.to(self.device)
                batch_targets = batch_targets.to(self.device)

                optimizer.zero_grad()
                outputs = model(batch_inputs)
                loss = criterion(outputs, batch_targets)
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()

                epoch_loss += loss.item()
                epoch_steps += 1
                global_step += 1

                # è®°å½•losså’Œperplexity
                if global_step % self.config.LOGGING_STEPS == 0:
                    avg_loss = epoch_loss / epoch_steps
                    perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')

                    self.losses.append(avg_loss)
                    self.perplexities.append(perplexity)

                    if self.logger:
                        self.logger.info(
                            f"æ­¥æ•° {global_step}: loss = {avg_loss:.4f}, å›°æƒ‘åº¦ = {perplexity:.4f}"
                        )

                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % self.config.SAVE_STEPS == 0:
                    self._save_checkpoint(
                        model, optimizer, vocab, global_step, epoch, model_dir
                    )

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
                if global_step >= self.config.MAX_STEPS:
                    break

            if global_step >= self.config.MAX_STEPS:
                break

            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_steps = 0

            with torch.no_grad():
                for batch_inputs, batch_targets in val_loader:
                    batch_inputs = batch_inputs.to(self.device)
                    batch_targets = batch_targets.to(self.device)

                    outputs = model(batch_inputs)
                    loss = criterion(outputs, batch_targets)

                    val_loss += loss.item()
                    val_steps += 1

            val_loss /= val_steps
            val_perplexity = math.exp(val_loss) if val_loss < 100 else float('inf')

            if self.logger:
                self.logger.info(
                    f"Epoch {epoch+1} - Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}"
                )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self._save_best_model(model, vocab, model_dir, val_loss, val_perplexity)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_final_model(model, vocab, model_dir)

        # ä¿å­˜è®­ç»ƒç»“æœ
        training_result = {
            "losses": self.losses,
            "perplexities": self.perplexities,
            "final_loss": self.losses[-1] if self.losses else None,
            "final_perplexity": self.perplexities[-1] if self.perplexities else None,
            "min_loss": min(self.losses) if self.losses else None,
            "min_perplexity": min(self.perplexities) if self.perplexities else None,
            "total_steps": global_step,
            "total_training_time": time.time() - self.start_time,
            "completed_at": datetime.now().isoformat()
        }

        result_file = os.path.join(model_dir, "training_results.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(training_result, f, indent=2)

        if self.logger:
            self.logger.info(f"è®­ç»ƒç»“æœä¿å­˜åˆ°: {result_file}")

        return {"losses": self.losses, "perplexities": self.perplexities}, training_result

    def _save_checkpoint(self, model, optimizer, vocab, step, epoch, model_dir):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä¸ä¿å­˜å®Œæ•´å†å²ä»¥èŠ‚çœå†…å­˜ï¼‰"""
        checkpoint_dir = os.path.join(model_dir, "checkpoints")
        os.makedirs(checkpoint_dir, exist_ok=True)

        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_step_{step}.pth")
        torch.save({
            'step': step,
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'vocab': vocab
        }, checkpoint_path)

    def _save_best_model(self, model, vocab, model_dir, val_loss, val_perplexity):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        best_model_path = os.path.join(model_dir, "best_model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'vocab': vocab,
            'val_loss': val_loss,
            'val_perplexity': val_perplexity
        }, best_model_path)

    def _save_final_model(self, model, vocab, model_dir):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        final_model_path = os.path.join(model_dir, "final_model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'vocab': vocab
        }, final_model_path)


# ========== ä¸»å‡½æ•° ==========


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""

    print("=" * 80)
    print("LSTM è¯­è¨€æ¨¡å‹è®­ç»ƒ - baby_data æ•°æ®é›†")
    print("=" * 80)

    # ç¯å¢ƒä¿¡æ¯
    print("\nğŸ“Š ç¯å¢ƒä¿¡æ¯:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")

    # é…ç½®
    config = TrainingConfig
    config.create_directories()

    # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    logger = setup_logging(config.LOG_FILE)
    logger.info("LSTM è¯­è¨€æ¨¡å‹è®­ç»ƒå¼€å§‹")
    logger.info(f"å·¥ä½œç›®å½•: {config.BASE_DIR}")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LSTMLanguageModelTrainer(config, logger=logger)

    # è®­ç»ƒæ‰€æœ‰æ•°æ®é›†
    all_metrics = {}

    for dataset_config in config.DATASETS:
        dataset_name = dataset_config['name']

        print(f"\n{'=' * 60}")
        print(f"è®­ç»ƒæ•°æ®é›†: {dataset_name}")
        print(f"æ•°æ®ç›®å½•: {dataset_config['data_dir']}")
        print(f"{'=' * 60}")

        # å¼€å§‹è®­ç»ƒè®¡æ—¶
        start_time = time.time()

        # è®­ç»ƒæ¨¡å‹
        training_data, train_result = trainer.train(dataset_config)

        # è®¡ç®—è®­ç»ƒæ—¶é—´
        training_time = time.time() - start_time

        if training_data is not None:
            losses = training_data.get("losses", [])
            perplexities = training_data.get("perplexities", [])

            # æ‰“å°è®­ç»ƒç»Ÿè®¡
            print(f"\nâœ… è®­ç»ƒå®Œæˆ - {dataset_name}:")
            if losses and perplexities:
                print(f"   æœ€ç»ˆloss: {losses[-1]:.4f}, æœ€ç»ˆå›°æƒ‘åº¦: {perplexities[-1]:.4f}")
                print(f"   æœ€å°loss: {min(losses):.4f}, æœ€å°å›°æƒ‘åº¦: {min(perplexities):.4f}")
                print(f"   è®­ç»ƒæ­¥æ•°: {len(losses)}")
                print(f"   è®­ç»ƒæ—¶é—´: {training_time / 60:.2f} åˆ†é’Ÿ")
            else:
                print("   æ²¡æœ‰è®­ç»ƒæ•°æ®")
        else:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {dataset_name}")

    # æ‰“å°æœ€ç»ˆæ±‡æ€»
    print(f"\n{'=' * 80}")
    print("è®­ç»ƒå®Œæˆæ±‡æ€»")
    print(f"{'=' * 80}")


if __name__ == '__main__':
    main()
