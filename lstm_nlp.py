#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LSTM è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ - è¯­è¨€å»ºæ¨¡è®­ç»ƒ
å‚è€ƒ colab_train_babydataset.py çš„ç»“æ„
åœ¨ data/ ç›®å½•çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè®°å½•losså’Œperplexity
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import math
import os
import json
import time
from datetime import datetime
from tqdm import tqdm
import glob


# ========== é…ç½®éƒ¨åˆ† ==========


class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""

    # åŸºç¡€è·¯å¾„é…ç½®
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    DATA_DIR = os.path.join(BASE_DIR, "data")
    RESULTS_DIR = os.path.join(BASE_DIR, "results")
    LOG_FILE = os.path.join(BASE_DIR, "training.log")

    # è®­ç»ƒé…ç½®
    BATCH_SIZE = 32
    NUM_EPOCHS = 10
    LEARNING_RATE = 0.001
    MAX_STEPS = 2000
    LOGGING_STEPS = 5
    SAVE_STEPS = 100

    # æ¨¡å‹é…ç½®
    EMBEDDING_DIM = 256
    HIDDEN_DIM = 512
    NUM_LAYERS = 2
    DROPOUT = 0.3
    MAX_LENGTH = 128

    # æ•°æ®é›†é…ç½®
    DATASETS = [
        {
            "name": "Natural Language",
            "data_file": os.path.join(DATA_DIR, "data_natural.txt"),
            "model_dir": os.path.join(RESULTS_DIR, "model_natural"),
            "file_pattern": "data_natural.txt"
        },
        {
            "name": "Impossible Language (Parity Negation)",
            "data_file": os.path.join(DATA_DIR, "impossible_output_parity_negation.txt"),
            "model_dir": os.path.join(RESULTS_DIR, "model_parity_negation"),
            "file_pattern": "impossible_output_parity_negation.txt"
        },
        {
            "name": "Impossible Language (Reversed)",
            "data_file": os.path.join(DATA_DIR, "impossible_output_reversed.txt"),
            "model_dir": os.path.join(RESULTS_DIR, "model_reversed"),
            "file_pattern": "impossible_output_reversed.txt"
        }
    ]

    @classmethod
    def create_directories(cls):
        """åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç›®å½•"""
        directories = [cls.BASE_DIR, cls.DATA_DIR, cls.RESULTS_DIR]
        print("åˆ›å»ºç›®å½•ç»“æ„:")
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            print(f"  âœ“ {directory}")

        for dataset in cls.DATASETS:
            os.makedirs(dataset["model_dir"], exist_ok=True)
            print(f"  âœ“ {dataset['name']}: {dataset['model_dir']}")


# ========== æ—¥å¿—é…ç½® ==========


def setup_logging(log_file):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    log_dir = os.path.dirname(log_file)
    if log_dir and not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    if logger.handlers:
        logger.handlers.clear()

    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger


import logging


# ========== æ•°æ®é›†ç±» ==========


class LanguageModelingDataset(Dataset):
    """è¯­è¨€å»ºæ¨¡æ•°æ®é›†ç±»"""

    def __init__(self, data_file, vocab, max_length=128):
        self.vocab = vocab
        self.max_length = max_length
        self.sequences = []

        # è¯»å–æ•°æ®
        with open(data_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # å¤„ç†æ¯è¡Œæ•°æ®
        for line in lines:
            line = line.strip()
            if not line:
                continue

            # åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰
            tokens = list(line)

            # è½¬æ¢ä¸ºç´¢å¼•
            indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]

            # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡åºåˆ—
            if len(indices) > 1:
                for i in range(len(indices) - 1):
                    input_seq = indices[:i+1]
                    target = indices[i+1]

                    # å¡«å……åˆ°å›ºå®šé•¿åº¦
                    if len(input_seq) < self.max_length:
                        input_seq += [self.vocab['<PAD>']] * (self.max_length - len(input_seq))
                    else:
                        input_seq = input_seq[-self.max_length:]

                    self.sequences.append((input_seq, target))

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        input_seq, target = self.sequences[idx]
        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)


# ========== LSTM è¯­è¨€æ¨¡å‹ ==========


class LSTMLanguageModel(nn.Module):
    """LSTM è¯­è¨€æ¨¡å‹"""

    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):
        super(LSTMLanguageModel, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=num_layers,
            dropout=dropout,
            batch_first=True
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        # x: [batch_size, seq_length]
        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]

        # LSTM å‰å‘ä¼ æ’­
        lstm_output, (hidden, cell) = self.lstm(embedded)

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_output[:, -1, :]  # [batch_size, hidden_dim]

        last_output = self.dropout(last_output)
        logits = self.fc(last_output)  # [batch_size, vocab_size]

        return logits


# ========== è®­ç»ƒç±» ==========


class LSTMLanguageModelTrainer:
    """LSTM è¯­è¨€æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, config, logger=None):
        self.config = config
        self.logger = logger
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.losses = []
        self.perplexities = []
        self.start_time = time.time()

    def build_vocab(self, data_file, min_freq=2):
        """æ„å»ºè¯æ±‡è¡¨"""
        from collections import Counter

        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter()
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    tokens = list(line)
                    word_freq.update(tokens)

        # æ„å»ºè¯æ±‡è¡¨
        vocab = {'<PAD>': 0, '<UNK>': 1}
        for word, freq in word_freq.items():
            if freq >= min_freq:
                vocab[word] = len(vocab)

        vocab_size = len(vocab)
        if self.logger:
            self.logger.info(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")

        return vocab

    def train(self, dataset_config):
        """è®­ç»ƒæ¨¡å‹"""

        dataset_name = dataset_config['name']
        data_file = dataset_config['data_file']
        model_dir = dataset_config['model_dir']

        if self.logger:
            self.logger.info(f"å¼€å§‹è®­ç»ƒ: {dataset_name}")
            self.logger.info(f"æ•°æ®æ–‡ä»¶: {data_file}")

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_file):
            if self.logger:
                self.logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return None, None

        # æ„å»ºè¯æ±‡è¡¨
        vocab = self.build_vocab(data_file)
        vocab_size = len(vocab)

        # åˆ›å»ºæ•°æ®é›†
        dataset = LanguageModelingDataset(
            data_file,
            vocab,
            max_length=self.config.MAX_LENGTH
        )

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_size = int(0.9 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.BATCH_SIZE,
            shuffle=True,
            num_workers=0
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.BATCH_SIZE,
            shuffle=False,
            num_workers=0
        )

        # åˆå§‹åŒ–æ¨¡å‹
        model = LSTMLanguageModel(
            vocab_size,
            self.config.EMBEDDING_DIM,
            self.config.HIDDEN_DIM,
            self.config.NUM_LAYERS,
            self.config.DROPOUT
        ).to(self.device)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
        optimizer = optim.Adam(model.parameters(), lr=self.config.LEARNING_RATE)

        # è®­ç»ƒå¾ªç¯
        global_step = 0
        best_val_loss = float('inf')

        for epoch in range(self.config.NUM_EPOCHS):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            epoch_loss = 0.0
            epoch_steps = 0

            for batch_inputs, batch_targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.config.NUM_EPOCHS}'):
                batch_inputs = batch_inputs.to(self.device)
                batch_targets = batch_targets.to(self.device)

                optimizer.zero_grad()
                outputs = model(batch_inputs)
                loss = criterion(outputs, batch_targets)
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()

                epoch_loss += loss.item()
                epoch_steps += 1
                global_step += 1

                # è®°å½•losså’Œperplexity
                if global_step % self.config.LOGGING_STEPS == 0:
                    avg_loss = epoch_loss / epoch_steps
                    perplexity = math.exp(avg_loss) if avg_loss < 100 else float('inf')

                    self.losses.append(avg_loss)
                    self.perplexities.append(perplexity)

                    if self.logger:
                        self.logger.info(
                            f"æ­¥æ•° {global_step}: loss = {avg_loss:.4f}, å›°æƒ‘åº¦ = {perplexity:.4f}"
                        )

                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if global_step % self.config.SAVE_STEPS == 0:
                    self._save_checkpoint(
                        model, optimizer, vocab, global_step, epoch, model_dir
                    )

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ­¥æ•°
                if global_step >= self.config.MAX_STEPS:
                    break

            if global_step >= self.config.MAX_STEPS:
                break

            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0.0
            val_steps = 0

            with torch.no_grad():
                for batch_inputs, batch_targets in val_loader:
                    batch_inputs = batch_inputs.to(self.device)
                    batch_targets = batch_targets.to(self.device)

                    outputs = model(batch_inputs)
                    loss = criterion(outputs, batch_targets)

                    val_loss += loss.item()
                    val_steps += 1

            val_loss /= val_steps
            val_perplexity = math.exp(val_loss) if val_loss < 100 else float('inf')

            if self.logger:
                self.logger.info(
                    f"Epoch {epoch+1} - Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.4f}"
                )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self._save_best_model(model, vocab, model_dir, val_loss, val_perplexity)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_final_model(model, vocab, model_dir)

        # ä¿å­˜è®­ç»ƒç»“æœ
        training_result = {
            "losses": self.losses,
            "perplexities": self.perplexities,
            "final_loss": self.losses[-1] if self.losses else None,
            "final_perplexity": self.perplexities[-1] if self.perplexities else None,
            "min_loss": min(self.losses) if self.losses else None,
            "min_perplexity": min(self.perplexities) if self.perplexities else None,
            "total_steps": global_step,
            "total_training_time": time.time() - self.start_time,
            "completed_at": datetime.now().isoformat()
        }

        result_file = os.path.join(model_dir, "training_results.json")
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(training_result, f, indent=2)

        if self.logger:
            self.logger.info(f"è®­ç»ƒç»“æœä¿å­˜åˆ°: {result_file}")

        return {"losses": self.losses, "perplexities": self.perplexities}, training_result

    def _save_checkpoint(self, model, optimizer, vocab, step, epoch, model_dir):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(model_dir, "checkpoints")
        os.makedirs(checkpoint_dir, exist_ok=True)

        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_step_{step}.pth")
        torch.save({
            'step': step,
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'vocab': vocab,
            'losses': self.losses,
            'perplexities': self.perplexities
        }, checkpoint_path)

    def _save_best_model(self, model, vocab, model_dir, val_loss, val_perplexity):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        best_model_path = os.path.join(model_dir, "best_model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'vocab': vocab,
            'val_loss': val_loss,
            'val_perplexity': val_perplexity
        }, best_model_path)

    def _save_final_model(self, model, vocab, model_dir):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        final_model_path = os.path.join(model_dir, "final_model.pth")
        torch.save({
            'model_state_dict': model.state_dict(),
            'vocab': vocab
        }, final_model_path)


# ========== ä¸»å‡½æ•° ==========


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""

    print("=" * 80)
    print("LSTM è¯­è¨€æ¨¡å‹è®­ç»ƒ - å‚è€ƒ colab_train_babydataset.py ç»“æ„")
    print("=" * 80)

    # ç¯å¢ƒä¿¡æ¯
    print("\nğŸ“Š ç¯å¢ƒä¿¡æ¯:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")

    # é…ç½®
    config = TrainingConfig
    config.create_directories()

    # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    logger = setup_logging(config.LOG_FILE)
    logger.info("LSTM è¯­è¨€æ¨¡å‹è®­ç»ƒå¼€å§‹")
    logger.info(f"å·¥ä½œç›®å½•: {config.BASE_DIR}")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LSTMLanguageModelTrainer(config, logger=logger)

    # è®­ç»ƒæ‰€æœ‰æ•°æ®é›†
    all_metrics = {}

    for dataset_config in config.DATASETS:
        dataset_name = dataset_config['name']

        print(f"\n{'=' * 60}")
        print(f"è®­ç»ƒæ•°æ®é›†: {dataset_name}")
        print(f"æ•°æ®æ–‡ä»¶: {dataset_config['data_file']}")
        print(f"{'=' * 60}")

        # å¼€å§‹è®­ç»ƒè®¡æ—¶
        start_time = time.time()

        # è®­ç»ƒæ¨¡å‹
        training_data, train_result = trainer.train(dataset_config)

        # è®¡ç®—è®­ç»ƒæ—¶é—´
        training_time = time.time() - start_time

        if training_data is not None:
            losses = training_data.get("losses", [])
            perplexities = training_data.get("perplexities", [])

            # æ‰“å°è®­ç»ƒç»Ÿè®¡
            print(f"\nâœ… è®­ç»ƒå®Œæˆ - {dataset_name}:")
            if losses and perplexities:
                print(f"   æœ€ç»ˆloss: {losses[-1]:.4f}, æœ€ç»ˆå›°æƒ‘åº¦: {perplexities[-1]:.4f}")
                print(f"   æœ€å°loss: {min(losses):.4f}, æœ€å°å›°æƒ‘åº¦: {min(perplexities):.4f}")
                print(f"   è®­ç»ƒæ­¥æ•°: {len(losses)}")
                print(f"   è®­ç»ƒæ—¶é—´: {training_time / 60:.2f} åˆ†é’Ÿ")
            else:
                print("   æ²¡æœ‰è®­ç»ƒæ•°æ®")
        else:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {dataset_name}")

    # æ‰“å°æœ€ç»ˆæ±‡æ€»
    print(f"\n{'=' * 80}")
    print("è®­ç»ƒå®Œæˆæ±‡æ€»")
    print(f"{'=' * 80}")


if __name__ == '__main__':
    main()